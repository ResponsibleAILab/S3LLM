An FQL sentence is comprised of a set of clauses. If there is one clause,
we simply return the query result of this clause. When there are multiple
clauses in a sentence, results from the various clauses will be summarized
by an FQL-provided command. A sentence with multiple clauses can be
expressed in the following form:
F QL command (Clause1, Clause2, ...) (1)
A clause is defined as a combination of phrases and FQL-reserved keywords. An example clause is listed as follows:
CHECK (keyword phrase) W HERE (f ile extension phrase) AS (feature name phrase) (2)
In the above grammar, CHECK, WHERE and AS are the reserved
keywords in FQL. They are not case sensitive. Note that a phrase is essentially a set of strings. The first version of FQL has three kinds of phrases: 1)
keyword phrase, 2) f ile extension phrase and 3) feature name phrase.


MPI_Put: Initiates a non-blocking put operation in one-sided MPI communication, transferring data from origin to target.

MPI_RPut: Initiates a put operation, potentially returning before operation completion, in one-sided MPI communication.

MPI_Get: Initiates a get operation in one-sided MPI communication, transferring data from target to origin.

MPI_RGet: Begins a non-blocking get operation, possibly completing before data transfer finishes, in one-sided MPI communication.

MPI_AINT_ADD: Adds two address integer values, useful in MPI for calculating displacements. Evaluated with a version context of 3.1.

MPI_AINT_DIFF: Computes the difference between two address integer values, used in addressing in MPI. Evaluated with a version context of 3.1.

MPI_COMM_DUP_WITH_INFO: Duplicates an MPI communicator with new info, allowing customization of communication properties. Evaluated with a version context of 3.0.

MPI_COMM_SET_INFO: Attaches new info to an existing MPI communicator, influencing communication optimizations. Evaluated with a version context of 3.0.

MPI_DIST_GRAPH_CREATE_ADJACENT: Creates a distributed graph communicator with specified neighbors for each node, enhancing data exchange patterns. Evaluated with a version context of 2.2.

MPI_DIST_GRAPH_CREATE: Constructs a distributed graph communicator based on arbitrary graphs, optimizing communication for irregular data patterns. Evaluated with a version context of 2.2.

mpi.h, use mpi, mpif.h: These terms refer to header files or modules used in C/C++ (mpi.h), Fortran 90 (use mpi), and Fortran 77 (mpif.h) MPI programming, indicating basic MPI support or inclusion. Evaluated with a version context of 2.0.

MPI_CART_Create: Creates a Cartesian grid communicator. Tagged as "Cartesian" to denote its purpose in creating communicators for Cartesian topologies.

MPI_GRAPH_Create: Initiates the creation of a graph communicator. Labeled as "Graph" to indicate its use in establishing communicators based on general graph topologies.

MPI_DIST_GRAPH_CREATE_Adjacent and MPI_DIST_GRAPH_Create: Both functions are concerned with the creation of distributed graph communicators, optimizing communication patterns among processes in distributed memory systems. Grouped under "Distributed Graph" to highlight their focus on creating communicators for distributed graph topologies

!$OMP: This syntax is typically used in Fortran to denote OpenMP directives. The "!" is a comment character in Fortran, and "$OMP" signifies the start of an OpenMP directive.

pragma omp: In C and C++, OpenMP directives are introduced with #pragma omp, indicating the compiler to process the following code block in parallel, according to the OpenMP standard.

schedule(static): Assigns iterations to threads in equal-sized chunks or according to a specified chunk size, in a round-robin fashion. Tagged as "Static" to indicate fixed distribution of iterations at runtime.

schedule(dynamic): Dynamically assigns a chunk of iterations to threads as they become available. Labeled as "Dynamic" to denote workload balancing at runtime, where the chunk size can be specified.

schedule(guided): Similar to dynamic scheduling, but the chunk size decreases over time to balance the workload among threads. Identified as "Guided" to reflect the adaptive allocation of iterations based on the remaining work.

schedule(auto): Leaves the decision of scheduling to the compiler or runtime system, which chooses the appropriate scheduling. Marked as "Auto" to indicate automatic selection of the scheduling strategy.

schedule(runtime): Allows the scheduling decision to be made at runtime using the OMP_SCHEDULE environment variable. Denoted as "Runtime" for its flexibility in choosing the scheduling strategy during program execution

omp task: Creates an explicit task that can be executed in parallel with tasks created by other threads. It marks the beginning of a block of code that will be executed by an available thread.

end task: While not a standalone directive in OpenMP, it conceptually represents the end of an omp task region. The actual syntax to end a task region is usually the closing brace of the block in C/C++ or end statement in Fortran, not a specific directive.

omp taskloop: Combines the tasking and work-sharing features by creating tasks for each iteration of a loop. It automatically divides the loop iterations into tasks.

omp taskloop simd: Further optimizes the execution by enabling the concurrent execution of multiple iterations of a loop in a single task using SIMD (Single Instruction, Multiple Data) instructions, where available.

omp taskyield: Provides a point at which a task can yield execution to allow other tasks to execute. This directive is useful for improving the balance and progress of task execution.

device: This attribute specifies a function or variable that is executed on the GPU, making it accessible from the device code.

global: Indicates a function that is callable from the host (CPU) and executed on the device (GPU). These functions are also known as kernel functions, which are launched by the host and executed in parallel by multiple threads on the GPU.

host: Specifies a function that is executed on the host (CPU) and callable from the host only. It is used to differentiate functions meant for execution on the CPU, especially in a context where functions can be executed on either the CPU or GPU.

noinline: A function qualifier that suggests to the compiler not to inline the specified function, providing more direct control over function calls and code optimization.

forceinline: Contrary to noinline, this attribute forces the compiler to inline the function whenever possible, aiming to reduce function call overhead at the expense of potentially larger binary size

cudaSetDevice: Sets the current GPU to be used for GPU execution. This function is crucial for directing operations to a specific GPU in systems with multiple GPUs.

cudaGetDeviceCount: Retrieves the number of compute-capable devices. This function is used to determine the number of GPUs available in the system, which is a fundamental step in managing resources in multi-GPU setups.

cudaGetDevice: Gets the device ID of the currently active GPU. This function allows for querying which GPU is currently being used for device operations, facilitating dynamic management of multiple GPUs

!$acc: This syntax is typically used in Fortran to denote OpenACC directives. The "!" is a comment character in Fortran, indicating a line of commentary, and "$acc" is used to specify the start of an OpenACC directive in the code. This form is used to annotate blocks of code for parallel execution or data management on accelerators.

#pragma acc: In C and C++, OpenACC directives are introduced with #pragma acc. This is used to mark sections of code for parallel execution, data movement, or optimization for accelerators. The #pragma acc directive enables the compiler to recognize and compile the specified code block for parallel execution on a supported accelerator

mpi.h, use mpi, mpif.h: These are used to include MPI functionality in C/C++ (mpi.h) and Fortran (use mpi for Fortran 90 and newer, mpif.h for Fortran 77) programs, respectively. The presence of these includes or modules indicates the use of MPI for distributed memory parallelism, allowing processes to communicate with each other, typically over a network.

#pragma omp: This is a directive used in C, C++, and Fortran programs to enable OpenMP, a parallel programming model for shared memory systems. It allows multiple threads to execute code concurrently on multiple cores of a processor, facilitating loop parallelism, task parallelism, and the management of parallel regions

MPI_FILE_OPEN: Opens a file for use by MPI processes, enabling parallel read and write operations.

MPI_FILE_DELETE: Removes a file from the file system, allowing for cleanup operations in parallel applications.

MPI_FILE_SET_SIZE: Sets the size of an MPI file, useful for preallocating space or adjusting file size as needed.

MPI_FILE_PREALLOCATE: Preallocates storage space for a file, optimizing file creation and size adjustment operations.

MPI_FILE_GET_SIZE: Retrieves the size of an MPI file, essential for managing data distribution and partitioning.

MPI_FILE_GET_GROUP: Obtains the group of processes that have the file open, facilitating coordination among processes.

MPI_FILE_GET_AMODE: Retrieves the access mode of the opened file, providing insights into how the file can be used by processes.

MPI_FILE_SET_INFO: Attaches an info object to a file, allowing for customization of I/O behavior through hints.

MPI_FILE_GET_INFO: Retrieves the info object associated with a file, offering visibility into current I/O settings and optimizations.

MPI_FILE_SET_VIEW: Establishes a new data view for a file, defining how data is partitioned and represented to processes.

MPI_FILE_GET_VIEW: Retrieves the current data view of a file, aiding in understanding data partitioning and access patterns.

CHECK (double) WHERE (.cu,.cuh) AS (double): This part checks for the usage of the double data type within CUDA files. The double data type represents double-precision floating-point numbers, offering higher precision at the cost of using more memory and potentially lower performance on some GPUs. Tagged as "double" to denote its double-precision characteristic.

CHECK (float) WHERE (.cu,.cuh) AS (single): This section evaluates the presence of the float data type in CUDA programming files. The float data type is used for single-precision floating-point numbers, which consume less memory and may offer faster computation on GPUs compared to double-precision types. Labeled as "single" to highlight its single-precision nature.

CHECK (stdnoreturn.h || stdalign.h) WHERE (*) AS (110): This checks for the presence of stdnoreturn.h and stdalign.h headers, which are associated with the C11 standard (indicated here as version 110). These headers introduce support for _Noreturn and alignment specifiers, respectively.

CHECK (stdbool.h || stdint.h) WHERE (*) AS (099): This evaluates the availability of stdbool.h and stdint.h, which are part of the C99 standard (represented as version 099). stdbool.h introduces the boolean data type, while stdint.h provides fixed-width integer types.

CHECK (float.h || limits.h || stdarg.h || stddef.h) WHERE (*) AS (089): This assesses the inclusion of float.h, limits.h, stdarg.h, and stddef.h, headers that are part of the C89/C90 standard (notated as version 089). These headers define limits of float types, sizes of basic types, variadic functions, and common definitions like size_t, respectively

CHECK (CODIMENSION) WHERE (.f,.f77,.f90,.f95,.f03) AS (108): Searches for the CODIMENSION feature, indicating support for the Fortran 2008 standard. CODIMENSION allows for the definition of co-arrays, a parallel programming extension in Fortran.

CHECK (class || select type) WHERE (.f,.f77,.f90,.f95,.f03) AS (103): Looks for usage of class and select type constructs, which are part of the Fortran 2003 standard. These constructs support polymorphism and type selection.

CHECK (FORALL || INDEPENDENT) WHERE (.f,.f77,.f90,.f95,.f03) AS (095): Evaluates the presence of the FORALL construct or INDEPENDENT directive, associated with Fortran 95. FORALL is used for array operations and INDEPENDENT hints at the independence of iterations for optimization.

CHECK (end module) WHERE (.f,.f77,.f90,.f95,.f03) AS (090): Checks for the end module statement, indicative of the Fortran 90 standard. Modules in Fortran 90 allow for better data encapsulation and code organization.

CHECK (implicit none || end program) WHERE (.f,.f77,.f90,.f95,.f03) AS (077): Searches for the implicit none statement or the end program structure, pointing to Fortran 77 compatibility. implicit none is used to enforce explicit type declarations.

CHECK (immintrin.h) WHERE (*) AS (SSEorAVX): searches for the inclusion of the immintrin.h header in code, indicating use of Intel's SSE (Streaming SIMD Extensions) or AVX (Advanced Vector Extensions) for parallel computing optimizations. This header provides intrinsic functions for SIMD operations, enhancing performance on compatible hardware

OpenMP 2.0: An API version introducing more parallelism control, including dynamic threads and task scheduling.

parallel for: Directive that parallelizes loops, distributing iterations across multiple threads efficiently.

barrier: Synchronization point where each thread waits until all threads have reached this point.

OpenMP 3.0: Enhanced version supporting task-based parallelism, allowing more flexible and dynamic work sharing.

task: Defines an independent unit of work that can be executed concurrently with other tasks.

collapse: Combines nested loops into a single loop to increase parallelism granularity.

OpenMP 3.1: Minor update, introducing features like atomic constructs improvements and non-rectangular loop support.

atomic capture: Ensures atomicity of a read-modify-write operation with optional value capture.

OpenMP 4.0: Major update introducing device offloading, enabling computations on GPUs and other accelerators.

target: Specifies code regions for execution on a device, like a GPU, for computational offloading.

teams: Creates a team of threads where each team is executed by a different team master.

distribute: Distributes loop iterations across the teams of threads, often used with 'target'.

simd: Directs the compiler to vectorize the specified loop, enhancing data parallelism.

OpenMP 4.5: Updated version, improving device support, introducing taskloops, and refining memory model.

taskloop: Combines the tasking and work-sharing constructs, generating tasks for each iteration of a loop.

Device constructs enhancements: Refinements in device offloading, improving usability and flexibility in heterogeneous architectures.

OpenMP 5.0: Update adding features like metadirectives, interoperability with other languages, and more explicit SIMD support.

declare variant: Allows defining function variants for specific contexts, enhancing performance portability.

scan: Provides a directive for inclusive and exclusive prefix reductions, useful in parallel algorithms.

requires: Specifies certain requirements that must be fulfilled by the OpenMP implementation.

OpenMP 5.1: Latest version with new features like 'detach' for task detachment, tile loops, and concurrent loop ordering.

detach: Enables the detachment of a task from its parent task region, allowing independent completion.

tile: Provides a directive for tiling loops, which can improve cache usage and performance on many cores.

order(concurrent): Allows unordered execution of iterations in a parallel loop, enabling optimizations like vectorization.